# -*- coding: utf-8 -*-
"""Linear_Regression,Ridge and Lasso..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13-DTT0ahXT8IjlZDLHYYrPMkecb26bHF

**Linear Regression, Ridge and Lasso**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.datasets import fetch_california_housing

df = fetch_california_housing()

print(df)

#converting into dataframe
dataset=pd.DataFrame(df.data)
dataset.columns=df.feature_names
dataset.head() #will show you independent feature

dataset['Price']=df.target
dataset.head() #this will be my dependent feature

"""**Lets divide the data into independent and dependent features**"""

# x i will be using as my independent feature
# to do that from all the columns we will skip the last column
x = dataset.iloc[:,:-1]
y=dataset.iloc[:,-1] #this will be my dependent feature

x.head()

y.head()

"""**Our first algo linear regression**"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
lin_reg = LinearRegression() #model configuration
mse = cross_val_score(lin_reg,x,y,scoring='neg_mean_squared_error',cv=6)
# since i have to take the mean of all 6 cross validation
mean_mse = np.mean(mse)
print(mean_mse)

"""**Ridge Regression**"""

from sklearn.linear_model import Ridge
# for doing hyperparameter tuning we will use gridsearchcv
from sklearn.model_selection import GridSearchCV
ridge = Ridge()

params={'alpha': [1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20]}

ridge_regressor = GridSearchCV(ridge,params,scoring='neg_mean_squared_error',cv=9)
ridge_regressor.fit(x,y)

print(ridge_regressor.best_params_)
print(ridge_regressor.best_score_)

"""**With lasso regression**"""

from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV
lasso=Lasso()

params={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20]}
lasso_regressor=GridSearchCV(lasso,params,scoring='neg_mean_squared_error',cv=6)
lasso_regressor.fit(x,y)

print(lasso_regressor.best_params_)
print(lasso_regressor.best_score_)

"""**Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_breast_cancer

# creating the independent features and dependent features
df=load_breast_cancer()
# independent features
X=pd.DataFrame(df['data'], columns=df['feature_names'])

X.head()

# Dependent features:
y=pd.DataFrame(df['target'], columns=["Target"])
print(y)

y['Target'].value_counts()

# train test split
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=42)

params=[{'C': [1,5,10]},{'max_iter':[100,150]}]

model1=LogisticRegression(C=100,max_iter=100)

GridSearchCV(model1,param_grid=params,scoring='f1',cv=5)

model1.fit(X_train,y_train)

print(model1.get_params())



